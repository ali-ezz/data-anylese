{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and Feature Engineering Notebook\n",
    "\n",
    "## Assignment Task 4: Data Cleaning & Feature Engineering\n",
    "\n",
    "This notebook covers comprehensive data cleaning and feature engineering including:\n",
    "- Handling missing values\n",
    "- Removing duplicates\n",
    "- Encoding categorical variables\n",
    "- Scaling numerical variables\n",
    "- Outlier detection and treatment\n",
    "- Feature creation and transformation\n",
    "- Data visualization of cleaning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Import our custom modules\n",
    "from data.data_loader import DataLoader\n",
    "from data.data_cleaning import DataCleaner, FeatureEngineer\n",
    "from visualization.plots import DataVisualizer\n",
    "\n",
    "# Import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Examine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data loader\n",
    "loader = DataLoader('../data/raw')\n",
    "\n",
    "# Load dataset (replace with your actual dataset)\n",
    "try:\n",
    "    # Example: dataset = loader.load_dataset('your_dataset.csv')\n",
    "    print(\"Please load your dataset using loader.load_dataset('filename.csv')\")\n",
    "    print(\"For now, using sample data for demonstration.\")\n",
    "    \n",
    "    # Sample dataset with various data quality issues\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    \n",
    "    sample_data = {\n",
    "        'id': range(1, n_samples + 1),\n",
    "        'age': np.random.normal(35, 10, n_samples),\n",
    "        'income': np.random.lognormal(10, 1, n_samples),\n",
    "        'education': np.random.choice(['High School', 'Bachelor', 'Master', 'PhD', None], n_samples, \n",
    "                                    p=[0.3, 0.4, 0.2, 0.05, 0.05]),\n",
    "        'city': np.random.choice(['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix'], n_samples),\n",
    "        'experience': np.random.normal(8, 5, n_samples),\n",
    "        'satisfaction': np.random.uniform(1, 10, n_samples),\n",
    "        'department': np.random.choice(['Sales', 'Marketing', 'Engineering', 'HR', 'Finance'], n_samples),\n",
    "        'target': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])\n",
    "    }\n",
    "    \n",
    "    # Add some missing values\n",
    "    missing_indices = np.random.choice(n_samples, 100, replace=False)\n",
    "    for idx in missing_indices[:50]:\n",
    "        sample_data['income'][idx] = np.nan\n",
    "    for idx in missing_indices[50:80]:\n",
    "        sample_data['age'][idx] = np.nan\n",
    "    for idx in missing_indices[80:]:\n",
    "        sample_data['satisfaction'][idx] = np.nan\n",
    "    \n",
    "    # Add some duplicate rows\n",
    "    duplicate_indices = np.random.choice(n_samples, 20, replace=False)\n",
    "    for idx in duplicate_indices:\n",
    "        sample_data['id'][idx] = np.random.choice(sample_data['id'][:idx] if idx > 0 else [1])\n",
    "    \n",
    "    # Add some outliers\n",
    "    outlier_indices = np.random.choice(n_samples, 10, replace=False)\n",
    "    for idx in outlier_indices[:5]:\n",
    "        sample_data['income'][idx] = sample_data['income'][idx] * 10  # Extreme high values\n",
    "    for idx in outlier_indices[5:]:\n",
    "        sample_data['age'][idx] = -10  # Invalid negative age\n",
    "    \n",
    "    dataset = pd.DataFrame(sample_data)\n",
    "    loader.dataset = dataset\n",
    "    loader._extract_dataset_info()\n",
    "    \n",
    "    print(f\"Dataset loaded with {len(dataset)} rows and {len(dataset.columns)} columns\")\n",
    "    print(f\"Initial missing values: {dataset.isnull().sum().sum()}\")\n",
    "    print(f\"Initial duplicates: {dataset.duplicated().sum()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 50)\n",
    "print(\"INITIAL DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dataset summary\n",
    "dataset_summary = loader.get_dataset_summary()\n",
    "print(f\"Dataset Size: {dataset_summary['dataset_size']}\")\n",
    "print(f\"Memory Usage: {dataset_summary['memory_usage']}\")\n",
    "print(f\"Numeric Variables: {dataset_summary['numeric_variables']}\")\n",
    "print(f\"Categorical Variables: {dataset_summary['categorical_variables']}\")\n",
    "print(f\"Total Missing Values: {dataset_summary['total_missing_values']}\")\n",
    "\n",
    "# Missing data analysis\n",
    "print(\"\\nMISSING DATA ANALYSIS:\")\n",
    "print(\"-\" * 25)\n",
    "missing_data = pd.DataFrame({\n",
    "    'Missing Count': loader.dataset.isnull().sum(),\n",
    "    'Missing Percentage': (loader.dataset.isnull().sum() / len(loader.dataset)) * 100\n",
    "})\n",
    "missing_data = missing_data[missing_data['Missing Count'] > 0].sort_values('Missing Percentage', ascending=False)\n",
    "if missing_data.empty:\n",
    "    print(\"No missing data found.\")\n",
    "else:\n",
    "    print(missing_data)\n",
    "\n",
    "# Duplicate analysis\n",
    "print(f\"\\nDUPLICATE ROWS: {loader.dataset.duplicated().sum()}\")\n",
    "\n",
    "# Data types\n",
    "print(\"\\nDATA TYPES:\")\n",
    "print(\"-\" * 12)\n",
    "print(loader.dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Data Cleaning Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize data cleaner and feature engineer\n",
    "cleaner = DataCleaner(loader.dataset)\n",
    "engineer = FeatureEngineer(loader.dataset)\n",
    "visualizer = DataVisualizer(loader.dataset)\n",
    "\n",
    "print(\"Data cleaning components initialized successfully.\")\n",
    "print(f\"Dataset shape: {loader.dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Handle missing values using auto strategy\n",
    "print(\"\\n1. APPLYING AUTO MISSING VALUE STRATEGY\")\n",
    "print(\"-\" * 40)\n",
    "cleaner.handle_missing_values(strategy='auto')\n",
    "\n",
    "# Show cleaning log\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(\"Cleaning operations performed:\")\n",
    "print(cleaning_report)\n",
    "\n",
    "# Visualize missing data before and after\n",
    "print(\"\\n2. MISSING DATA VISUALIZATION\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Before cleaning\n",
    "missing_before = loader.dataset.isnull().sum()\n",
    "missing_after = cleaner.dataset.isnull().sum()\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "missing_before[missing_before > 0].plot(kind='bar', ax=ax1, color='red', alpha=0.7)\n",
    "ax1.set_title('Missing Values Before Cleaning')\n",
    "ax1.set_xlabel('Columns')\n",
    "ax1.set_ylabel('Missing Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "missing_after[missing_after > 0].plot(kind='bar', ax=ax2, color='green', alpha=0.7)\n",
    "ax2.set_title('Missing Values After Cleaning')\n",
    "ax2.set_xlabel('Columns')\n",
    "ax2.set_ylabel('Missing Count')\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Missing values reduced from {loader.dataset.isnull().sum().sum()} to {cleaner.dataset.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Remove Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 30)\n",
    "print(\"REMOVING DUPLICATES\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Remove duplicates\n",
    "print(f\"\\nDuplicate rows before cleaning: {cleaner.dataset.duplicated().sum()}\")\n",
    "cleaner.remove_duplicates()\n",
    "print(f\"Duplicate rows after cleaning: {cleaner.dataset.duplicated().sum()}\")\n",
    "\n",
    "# Update cleaning log\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(\"\\nUpdated cleaning operations:\")\n",
    "print(cleaning_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Handle Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 25)\n",
    "print(\"HANDLING OUTLIERS\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Detect outliers before removal\n",
    "numeric_columns = cleaner.dataset.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNUMERICAL COLUMNS: {numeric_columns}\")\n",
    "\n",
    "# Visualize outliers using boxplots\n",
    "print(\"\\n1. OUTLIER VISUALIZATION BEFORE CLEANING\")\n",
    "print(\"-\" * 42)\n",
    "if len(numeric_columns) > 0:\n",
    "    n_cols = min(4, len(numeric_columns))\n",
    "    n_rows = (len(numeric_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_columns):\n",
    "        if i < len(axes):\n",
    "            cleaner.dataset.boxplot(column=col, ax=axes[i])\n",
    "            axes[i].set_title(f'{col} - Before Outlier Removal')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Remove outliers using IQR method\n",
    "print(\"\\n2. REMOVING OUTLIERS USING IQR METHOD\")\n",
    "print(\"-\" * 38)\n",
    "initial_rows = len(cleaner.dataset)\n",
    "cleaner.remove_outliers(method='iqr', threshold=1.5)\n",
    "final_rows = len(cleaner.dataset)\n",
    "removed_outliers = initial_rows - final_rows\n",
    "\n",
    "print(f\"Rows before outlier removal: {initial_rows}\")\n",
    "print(f\"Rows after outlier removal: {final_rows}\")\n",
    "print(f\"Outliers removed: {removed_outliers}\")\n",
    "\n",
    "# Visualize outliers after removal\n",
    "print(\"\\n3. OUTLIER VISUALIZATION AFTER CLEANING\")\n",
    "print(\"-\" * 39)\n",
    "if len(numeric_columns) > 0:\n",
    "    n_cols = min(4, len(numeric_columns))\n",
    "    n_rows = (len(numeric_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numeric_columns):\n",
    "        if i < len(axes) and col in cleaner.dataset.columns:\n",
    "            cleaner.dataset.boxplot(column=col, ax=axes[i])\n",
    "            axes[i].set_title(f'{col} - After Outlier Removal')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numeric_columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Update cleaning log\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(\"\\nUpdated cleaning operations:\")\n",
    "print(cleaning_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = cleaner.dataset.select_dtypes(include=['object']).columns.tolist()\n",
    "print(f\"\\nCATEGORICAL COLUMNS: {categorical_columns}\")\n",
    "\n",
    "# Show unique values for each categorical column\n",
    "print(\"\\nUNIQUE VALUES IN CATEGORICAL COLUMNS:\")\n",
    "print(\"-\" * 40)\n",
    "for col in categorical_columns:\n",
    "    unique_vals = cleaner.dataset[col].nunique()\n",
    "    print(f\"{col}: {unique_vals} unique values\")\n",
    "    if unique_vals <= 10:  # Show values for low cardinality\n",
    "        print(f\"  Values: {list(cleaner.dataset[col].unique())}\")\n",
    "\n",
    "# Apply label encoding\n",
    "print(\"\\nAPPLYING LABEL ENCODING\")\n",
    "print(\"-\" * 25)\n",
    "cleaner.encode_categorical_variables(method='label')\n",
    "\n",
    "# Show data types after encoding\n",
    "print(\"\\nDATA TYPES AFTER ENCODING:\")\n",
    "print(\"-\" * 28)\n",
    "print(cleaner.dataset.dtypes)\n",
    "\n",
    "# Update cleaning log\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(\"\\nUpdated cleaning operations:\")\n",
    "print(cleaning_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scale Numerical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 35)\n",
    "print(\"SCALING NUMERICAL VARIABLES\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Identify numerical columns\n",
    "numerical_columns = cleaner.dataset.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNUMERICAL COLUMNS: {numerical_columns}\")\n",
    "\n",
    "# Show statistics before scaling\n",
    "print(\"\\nSTATISTICS BEFORE SCALING:\")\n",
    "print(\"-\" * 28)\n",
    "print(cleaner.dataset[numerical_columns].describe())\n",
    "\n",
    "# Apply standard scaling\n",
    "print(\"\\nAPPLYING STANDARD SCALING\")\n",
    "print(\"-\" * 25)\n",
    "cleaner.scale_numerical_variables(method='standard')\n",
    "\n",
    "# Show statistics after scaling\n",
    "print(\"\\nSTATISTICS AFTER SCALING:\")\n",
    "print(\"-\" * 27)\n",
    "print(cleaner.dataset[numerical_columns].describe())\n",
    "\n",
    "# Visualize scaling effect\n",
    "print(\"\\nVISUALIZING SCALING EFFECT\")\n",
    "print(\"-\" * 27)\n",
    "if len(numerical_columns) > 0:\n",
    "    n_cols = min(4, len(numerical_columns))\n",
    "    n_rows = (len(numerical_columns) + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    if n_rows == 1 and n_cols == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1 or n_cols == 1:\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(numerical_columns):\n",
    "        if i < len(axes):\n",
    "            cleaner.dataset[col].hist(bins=30, ax=axes[i], alpha=0.7, color='skyblue')\n",
    "            axes[i].set_title(f'{col} - After Scaling')\n",
    "            axes[i].set_xlabel('Value')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(len(numerical_columns), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Update cleaning log\n",
    "cleaning_report = cleaner.get_cleaning_report()\n",
    "print(\"\\nUpdated cleaning operations:\")\n",
    "print(cleaning_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 30)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Initialize feature engineer with cleaned data\n",
    "engineer = FeatureEngineer(cleaner.dataset)\n",
    "\n",
    "# Identify numerical columns for feature engineering\n",
    "numerical_columns = engineer.dataset.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNUMERICAL COLUMNS FOR FEATURE ENGINEERING: {numerical_columns}\")\n",
    "\n",
    "# Create interaction features (example)\n",
    "print(\"\\n1. CREATING INTERACTION FEATURES\")\n",
    "print(\"-\" * 33)\n",
    "if len(numerical_columns) >= 2:\n",
    "    # Create interaction between first two numerical columns\n",
    "    column_pairs = [(numerical_columns[0], numerical_columns[1])]\n",
    "    engineer.create_interaction_features(column_pairs)\n",
    "    print(f\"Created interaction feature: {numerical_columns[0]}_x_{numerical_columns[1]}\")\n",
    "\n",
    "# Create binned features (example)\n",
    "print(\"\\n2. CREATING BINNED FEATURES\")\n",
    "print(\"-\" * 27)\n",
    "if len(numerical_columns) > 0:\n",
    "    # Create binned feature for the first numerical column\n",
    "    engineer.create_binned_features([numerical_columns[0]], bins=5)\n",
    "    print(f\"Created binned feature: {numerical_columns[0]}_binned\")\n",
    "\n",
    "# Show feature engineering report\n",
    "feature_report = engineer.get_feature_report()\n",
    "print(\"\\nFeature engineering operations:\")\n",
    "print(feature_report)\n",
    "\n",
    "# Show final dataset info\n",
    "print(\"\\nFINAL DATASET INFORMATION:\")\n",
    "print(\"-\" * 26)\n",
    "print(f\"Shape: {engineer.dataset.shape}\")\n",
    "print(f\"Columns: {list(engineer.dataset.columns)}\")\n",
    "print(f\"Data types:\\n{engineer.dataset.dtypes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Cleaned and Engineered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 40)\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Save cleaned and engineered dataset\n",
    "try:\n",
    "    # Create processed data directory if it doesn't exist\n",
    "    os.makedirs('../data/processed', exist_ok=True)\n",
    "    \n",
    "    # Save the final processed dataset\n",
    "    engineer.dataset.to_csv('../data/processed/cleaned_dataset.csv', index=False)\n",
    "    print(\"✓ Cleaned dataset saved to data/processed/cleaned_dataset.csv\")\n",
    "    \n",
    "    # Save cleaning report\n",
    "    with open('../reports/cleaning_report.txt', 'w') as f:\n",
    "        f.write(\"DATA CLEANING REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(cleaner.get_cleaning_report())\n",
    "        f.write(\"\\n\\nFEATURE ENGINEERING REPORT\\n\")\n",
    "        f.write(\"=\" * 50 + \"\\n\\n\")\n",
    "        f.write(engineer.get_feature_report())\n",
    "    \n",
    "    print(\"✓ Cleaning and feature engineering reports saved to reports/cleaning_report.txt\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error saving data: {e}\")\n",
    "\n",
    "# Show final summary\n",
    "print(\"\\nFINAL SUMMARY:\")\n",
    "print(\"-\" * 15)\n",
    "print(f\"Original dataset shape: {loader.dataset.shape}\")\n",
    "print(f\"Final dataset shape: {engineer.dataset.shape}\")\n",
    "print(f\"Rows removed: {loader.dataset.shape[0] - engineer.dataset.shape[0]}\")\n",
    "print(f\"Columns added: {engineer.dataset.shape[1] - loader.dataset.shape[1]}\")\n",
    "print(f\"Missing values in final dataset: {engineer.dataset.isnull().sum().sum()}\")\n",
    "print(f\"Duplicate rows in final dataset: {engineer.dataset.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Findings and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 45)\n",
    "print(\"KEY FINDINGS AND RECOMMENDATIONS\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(\"\"\"\n",
    "KEY FINDINGS:\n",
    "-------------\n",
    "1. Data Quality Issues Addressed:\n",
    "   - Missing values: Successfully handled using appropriate strategies\n",
    "   - Duplicate records: Removed to ensure data integrity\n",
    "   - Outliers: Identified and removed using IQR method\n",
    "   - Inconsistent data types: Standardized through encoding\n",
    "\n",
    "2. Data Transformation Results:\n",
    "   - Categorical variables encoded for machine learning compatibility\n",
    "   - Numerical variables scaled for better model performance\n",
    "   - New features created through engineering techniques\n",
    "   - Dataset standardized for consistent analysis\n",
    "\n",
    "3. Dataset Improvements:\n",
    "   - Improved data quality and consistency\n",
    "   - Enhanced feature set for better model performance\n",
    "   - Ready for machine learning algorithms\n",
    "   - Properly documented cleaning process\n",
    "\n",
    "RECOMMENDATIONS:\n",
    "----------------\n",
    "1. For Production Use:\n",
    "   - Implement more sophisticated outlier detection methods\n",
    "   - Consider domain-specific imputation strategies\n",
    "   - Validate engineered features with domain experts\n",
    "   - Implement automated data quality monitoring\n",
    "\n",
    "2. For Model Development:\n",
    "   - Use the cleaned dataset for model training\n",
    "   - Consider feature selection techniques\n",
    "   - Validate scaling methods with cross-validation\n",
    "   - Document all preprocessing steps for reproducibility\n",
    "\n",
    "3. For Future Improvements:\n",
    "   - Implement more advanced feature engineering techniques\n",
    "   - Add data validation and quality checks\n",
    "   - Consider automated hyperparameter tuning for scaling\n",
    "   - Implement data versioning and lineage tracking\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nDATA CLEANING AND FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
